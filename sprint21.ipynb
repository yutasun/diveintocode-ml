{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インポート\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "b = np.array([1, 1, 1, 1]) # (n_nodes,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01, 0.02]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.01 0.02]\n",
      "  [0.02 0.03]\n",
      "  [0.03 0.04]]]\n"
     ]
    }
   ],
   "source": [
    "print(x[:1:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma=0.005):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        pass\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.rand(n_nodes2)\n",
    "        pass\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.001):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        layer.W = layer.W - self.lr*layer.dW\n",
    "        layer.B = layer.B - self.lr*layer.dB.mean(axis=0)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# タンジェントハイポ\n",
    "class tanh:\n",
    "    def __init__(self):\n",
    "        \n",
    "        pass\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = np.tanh(A)\n",
    "        self.Z = Z       \n",
    "        return Z        \n",
    "    def backward(self, dZ):\n",
    "        dA = dZ*(1-np.tanh(self.A)**2)\n",
    "    \n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ソフトマックス\n",
    "class softmax:\n",
    "    def __init__(self):\n",
    "        \n",
    "        pass\n",
    "    def forward(self, A):\n",
    "        Z = (np.exp(A).T/np.exp(A).sum(axis=1)).T\n",
    "         \n",
    "        return Z\n",
    "    def backward(self, Z, Y):\n",
    "        dA = Z - Y\n",
    "        \n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x, w_x,w_h, batch_size,b,initializer=SimpleInitializer, optimizer=SGD):\n",
    "        self.initializer = SimpleInitializer\n",
    "        self.optimizer = optimizer\n",
    "        self.n_sequences = x.shape[1]\n",
    "        self.n_features = x.shape[2]\n",
    "        self.n_nodes = w_x.shape[1]\n",
    "        \n",
    "        ini = self.initializer()\n",
    "        #初期値を決める\n",
    "        # WとBを決定する\n",
    "        self.Wx = w_x\n",
    "        self.Wh = w_h\n",
    "        self.B = b\n",
    "        self.h = np.zeros((batch_size, self.n_nodes))\n",
    "        \n",
    "        opt = self.optimizer\n",
    "        #最適化処理をインスタンス化\n",
    "        self.activation1 = tanh()\n",
    "        \n",
    "        pass\n",
    "    def forward(self, Z):\n",
    "        self.Z = Z\n",
    "        A = self.Z@self.Wx + self.h@self.Wh + self.B\n",
    "        self.A = A\n",
    "        \n",
    "        self.h = self.activation1.forward(A)\n",
    "        \n",
    "        pass\n",
    "        return A\n",
    "    def backward(self, dA):\n",
    "        self.dA = dA\n",
    "        self.dB = self.dA.mean(axis=0)\n",
    "        self.dW = self.Z.T@self.dA\n",
    "        dZ = self.dA@self.W.T\n",
    "        \n",
    "        pass\n",
    "        # 更新\n",
    "        self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchSimpleRNNClassifier():\n",
    "\n",
    "    def __init__(self,num_iter = 15,lr = 0.01, verbose = True):\n",
    "        self.verbose = verbose\n",
    "        self.num_iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "#         self.n_features = 784\n",
    "#         self.n_nodes1 = 400\n",
    "#         self.n_nodes2 = 200\n",
    "        self.n_output = 10\n",
    "        self.alpha = 0.001\n",
    "        self.rec_loss = [] \n",
    "        self.rec_val_loss = []         \n",
    "        \n",
    "        pass\n",
    "    def fit(self, x, w_x,w_h, batch_size,b):\n",
    "        \n",
    "        #最適化手法のインスタンス化\n",
    "        optimizer = SGD(self.lr)\n",
    "        initializer = SimpleInitializer()\n",
    "        #層の数、各層の特徴数（インプット＆アウトプット）、初期値設定、最適化手法、活性化関数の種類を設定\n",
    "        self.FC1 = FC(x, w_x,w_h, batch_size,b,initializer, optimizer)\n",
    "#         self.activation1 = tanh()\n",
    "#         self.FC2 = FC(self.n_nodes1, self.n_nodes2, initializer, optimizer)\n",
    "#         self.activation2 = tanh()\n",
    "#         self.FC3 = FC(self.n_nodes2, self.n_output, initializer, optimizer)\n",
    "        self.activation3 = softmax()\n",
    "\n",
    "        self.X = x\n",
    "#         self.y = y\n",
    "#         self.X_val = X_val\n",
    "#         self.y_val = y_val\n",
    "#         get_mini_batch = GetMiniBatch(self.X, self.y, batch_size=20)\n",
    "#         self.z_all = np.empty((0, 10),dtype=np.float)\n",
    "        for i in range(self.FC1.n_sequences):\n",
    "#             for mini_X_train, mini_y_train in get_mini_batch:\n",
    "        # フォワードプロパゲーション\n",
    "            A1 = self.FC1.forward(self.X[:,i,:])\n",
    "#                 Z1 = self.activation1.forward(A1)\n",
    "#                 A2 = self.FC2.forward(Z1)\n",
    "#                 Z2 = self.activation2.forward(A2)\n",
    "#                 A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A1)\n",
    "        self.Z3 = Z3\n",
    "#         # バックプロパゲーション\n",
    "#                 dA3 = self.activation3.backward(Z3, mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "#                 dZ2 = self.FC3.backward(dA3)\n",
    "#                 dA2 = self.activation2.backward(dZ2)\n",
    "#                 dZ1 = self.FC2.backward(dA2)\n",
    "#                 dA1 = self.activation1.backward(dZ1)\n",
    "#                 dZ0 = self.FC1.backward(dA1) # dZ0は使用しない\n",
    "#             self.loss_entropy(mini_y_train)\n",
    "#             #フィット後のB、Wを抜き出す\n",
    "#             self.B1 = self.FC1.B\n",
    "#             self.B2 = self.FC2.B\n",
    "#             self.B3 = self.FC3.B\n",
    "            \n",
    "#             self.W1 = self.FC1.W\n",
    "#             self.W2 = self.FC2.W\n",
    "#             self.W3 = self.FC3.W\n",
    "#             self.val_loss_entropy()\n",
    "        print(self.FC1.h)        \n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            print()\n",
    "        pass\n",
    "\n",
    "#     def loss_entropy(self,y_train_batch):\n",
    "#         self.loss = -1/self.Z3.shape[0]*(y_train_batch*(np.log(self.Z3))).sum()  \n",
    "#         self.rec_loss.append(self.loss)\n",
    "        \n",
    "#     def val_loss_entropy(self):\n",
    "#         self.a1 = self.X_val@self.W1 + self.B1\n",
    "#         self.z1 = np.tanh(self.a1)\n",
    "#         self.a2 = self.z1@self.W2 + self.B2\n",
    "#         self.z2 = np.tanh(self.a2)\n",
    "#         self.a3 = self.z2@self.W3 + self.B3\n",
    "#         self.z3 = self.activation3.forward(self.a3)        \n",
    "#         self.val_loss = -1/self.z3.shape[0]*(self.y_val*(np.log(self.z3))).sum()  \n",
    "#         self.rec_val_loss.append(self.val_loss)\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        z_all = np.empty((0, 10),dtype=np.float)\n",
    "#         print(z_all.shape)\n",
    "        self.a1 = X_test@self.W1 + self.B1\n",
    "        self.z1 = np.tanh(self.a1)\n",
    "        self.a2 = self.z1@self.W2 + self.B2\n",
    "        self.z2 = np.tanh(self.a2)\n",
    "        self.a3 = self.z2@self.W3 + self.B3\n",
    "        self.z3 = self.activation3.forward(self.a3)\n",
    "        z_all = np.concatenate([z_all,self.z3])\n",
    "        z_all_index = np.argmax(z_all, axis=1)\n",
    "        \n",
    "        pass\n",
    "        return z_all, z_all_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.79494228 0.81839002 0.83939649 0.85584174]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = ScratchSimpleRNNClassifier()\n",
    "clf.fit(x, w_x,w_h, batch_size,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
