{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】スクラッチを振り返る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでのスクラッチを振り返り、ディープラーニングを実装するためにはどのようなものが必要だったかを列挙してください。<br>\n",
    "（例）重みを初期化する必要があった<br>\n",
    "     エポックのループが必要だった<br>\n",
    "それらがフレームワークにおいてはどのように実装されるかを今回覚えていきましょう。<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ミニバッチで学習を回す\n",
    "- フォワードプロ、バックプロでW,Bの更新\n",
    "- LOSS値の算出\n",
    "- 学習率他、ハイパーパラメーターの設定\n",
    "- 畳み込みのチャネル、フィルターの設計"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データセットの用意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】スクラッチとTensorFlowの対応を考える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のサンプルコードを見て、先ほど列挙した「ディープラーニングを実装するために必要なもの」が<br>\n",
    "TensorFlowではどう実装されているかを確認してください。<br>\n",
    "それを簡単に言葉でまとめてください。単純な一対一の対応であるとは限りません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- バックプロパゲーションの計算式がない\n",
    "　optimizer.minimize(loss_op)で計算？\n",
    "- 交差エントロピーの計算自体はあるが計算式がない（推定結果を正解値の入力するだけ）\n",
    "- 出力結果はprobaで出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n",
      "Epoch 0, loss : 28.8249, val_loss : 62.5707, acc : 0.750, val_acc : 0.375\n",
      "Epoch 1, loss : 27.4720, val_loss : 59.5359, acc : 0.750, val_acc : 0.375\n",
      "Epoch 2, loss : 57.6751, val_loss : 28.2467, acc : 0.250, val_acc : 0.625\n",
      "Epoch 3, loss : 5.4079, val_loss : 13.2653, acc : 0.750, val_acc : 0.375\n",
      "Epoch 4, loss : 6.1407, val_loss : 9.5193, acc : 0.250, val_acc : 0.625\n",
      "Epoch 5, loss : 0.4262, val_loss : 4.7512, acc : 0.750, val_acc : 0.625\n",
      "Epoch 6, loss : 1.4025, val_loss : 6.9663, acc : 0.750, val_acc : 0.688\n",
      "Epoch 7, loss : 0.0187, val_loss : 3.9771, acc : 1.000, val_acc : 0.625\n",
      "Epoch 8, loss : 0.3151, val_loss : 5.9727, acc : 0.750, val_acc : 0.688\n",
      "Epoch 9, loss : 0.0065, val_loss : 3.9492, acc : 1.000, val_acc : 0.625\n",
      "test_acc : 0.800\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "# データセットの読み込み\n",
    "\n",
    "iris_data = load_iris()\n",
    "X = iris_data.data[50:]\n",
    "y = iris_data.target[50:] - 1\n",
    "\n",
    "# dataset_path =\"Iris.csv\"\n",
    "# df = pd.read_csv(dataset_path)\n",
    "# # データフレームから条件抽出\n",
    "# df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "# y = df[\"Species\"]\n",
    "# X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "# y = np.array(y)\n",
    "# X = np.array(X)\n",
    "# # ラベルを数値に変換\n",
    "# y[y=='Iris-versicolor'] = 0\n",
    "# y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01 #学習率\n",
    "batch_size = 10 #バッチサイズ\n",
    "num_epochs = 10 #エポック数\n",
    "n_hidden1 = 50 #２層目\n",
    "n_hidden2 = 100 #１層目\n",
    "n_input = X_train.shape[1] #特徴量\n",
    "n_samples = X_train.shape[0] #サンプル数\n",
    "n_classes = 1 #２値分類（１個のワンホット）\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input]) #（サンプル数未定、特徴量数）\n",
    "Y = tf.placeholder(\"float\", [None, n_classes]) #（サンプル数未定、クラス分類）\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size) #ミニバッチでデータを分割\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])), #重み（４、５０）\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])), #重み（５０、１００）\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes])) #重み（１００、３）\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])), #バイアス（５０）\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),#バイアス（１００）\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))#バイアス（３）\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1']) #X@W1 + b1\n",
    "    layer_1 = tf.nn.relu(layer_1) #reru\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2']) #X@W2 + b2\n",
    "    layer_2 = tf.nn.relu(layer_2) #reru\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] #X@W3 + b3\n",
    "    return layer_output\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X) #上記クラスの計算\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# 出力結果と正解値で交差エントロピー\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)#アダムグラッド？\n",
    "train_op = optimizer.minimize(loss_op)#クロスエントロピーの結果からバックプロパゲーション？\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))#正解、不正解のBoolを返す\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))#１、０に変換し正解率を計算\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()#W,Bの初期化\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)#初期化\n",
    "    for epoch in range(num_epochs):#エポック数分最適化を繰り返す\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int) #小数点以下切り下げ？でバッチのループ数計算\n",
    "        total_loss = 0 #ロスを初期化\n",
    "        total_acc = 0 #正解率を初期化\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):#ミニバッチの計算\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y}) \n",
    "            #最適化の数値としてtrain_opを指定、引数として辞書型のミニバッチ（）\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #ロスエントロピーとアキュラシーをX,Yに対して計算\n",
    "            total_loss += loss #ロスをトータルロスに追加 \n",
    "            total_acc += acc #アキュラシーをトータルアキュラシーに追加\n",
    "        total_loss /= n_samples #サンプル数で割る\n",
    "        total_acc /= n_samples #サンプル数で割る\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})#バリデーションでバルロスを計算\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "        # エポックごとにロス、バルロス、正解率をプリント\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})#テストデータで正解率を計算\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 他のデータセットへの適用\n",
    "- Iris（3種類全ての目的変数を使用）\n",
    "- House Prices\n",
    "- どのデータセットもtrain, val, testの3種類に分けて使用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】3種類全ての目的変数を使用したIrisのモデルを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 4)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データセットの読み込み\n",
    "iris_data = load_iris()\n",
    "X = iris_data.data[:]\n",
    "y = iris_data.target[:]\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "y = y_one_hot.astype(np.int)\n",
    "# print(y.shape)\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "np.argmax(y_train,1)\n",
    "\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 82.9003, val_loss : 45.5487, acc : 0.333, val_acc : 0.542\n",
      "Epoch 1, loss : 7.8293, val_loss : 10.5941, acc : 0.833, val_acc : 0.417\n",
      "Epoch 2, loss : 2.4431, val_loss : 6.6046, acc : 0.500, val_acc : 0.667\n",
      "Epoch 3, loss : 0.4123, val_loss : 4.5080, acc : 0.667, val_acc : 0.750\n",
      "Epoch 4, loss : 0.1229, val_loss : 4.7169, acc : 0.833, val_acc : 0.792\n",
      "Epoch 5, loss : 0.0064, val_loss : 3.9906, acc : 1.000, val_acc : 0.833\n",
      "Epoch 6, loss : 0.0061, val_loss : 3.4577, acc : 1.000, val_acc : 0.875\n",
      "Epoch 7, loss : 0.0093, val_loss : 3.2081, acc : 1.000, val_acc : 0.875\n",
      "Epoch 8, loss : 0.0089, val_loss : 3.0325, acc : 1.000, val_acc : 0.875\n",
      "Epoch 9, loss : 0.0051, val_loss : 3.0715, acc : 1.000, val_acc : 0.875\n",
      "test_acc : 0.933\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.002 #学習率\n",
    "batch_size = 10 #バッチサイズ\n",
    "num_epochs = 10 #エポック数\n",
    "n_hidden1 = 50 #２層目\n",
    "n_hidden2 = 100 #１層目\n",
    "n_input = X_train.shape[1] #特徴量\n",
    "n_samples = X_train.shape[0] #サンプル数\n",
    "n_classes = 3 #２値分類（１個のワンホット）\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input]) #（サンプル数未定、特徴量数）\n",
    "Y = tf.placeholder(\"float\", [None, n_classes]) #（サンプル数未定、クラス分類）\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size) #ミニバッチでデータを分割\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])), #重み（４、５０）\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])), #重み（５０、１００）\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes])) #重み（１００、３）\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])), #バイアス（５０）\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),#バイアス（１００）\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))#バイアス（３）\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1']) #X@W1 + b1\n",
    "    layer_1 = tf.nn.relu(layer_1) #reru\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2']) #X@W2 + b2\n",
    "    layer_2 = tf.nn.relu(layer_2) #reru\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] #X@W3 + b3\n",
    "    return layer_output\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X) #上記クラスの計算\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "# 出力結果と正解値で交差エントロピー\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)#アダムグラッド？\n",
    "train_op = optimizer.minimize(loss_op)#クロスエントロピーの結果からバックプロパゲーション？\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.argmax(Y,1),tf.argmax(logits,1))#正解、不正解のBoolを返す\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))#１、０に変換し正解率を計算\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()#W,Bの初期化\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)#初期化\n",
    "    for epoch in range(num_epochs):#エポック数分最適化を繰り返す\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int) #小数点以下切り下げ？でバッチのループ数計算\n",
    "        total_loss = 0 #ロスを初期化\n",
    "        total_acc = 0 #正解率を初期化\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):#ミニバッチの計算\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y}) \n",
    "            #最適化の数値としてtrain_opを指定、引数として辞書型のミニバッチ（）\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #ロスエントロピーとアキュラシーをX,Yに対して計算\n",
    "            total_loss += loss #ロスをトータルロスに追加 \n",
    "            total_acc += acc #アキュラシーをトータルアキュラシーに追加\n",
    "        total_loss /= n_samples #サンプル数で割る\n",
    "        total_acc /= n_samples #サンプル数で割る\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})#バリデーションでバルロスを計算\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "        # エポックごとにロス、バルロス、正解率をプリント\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})#テストデータで正解率を計算\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】House Pricesのモデルを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.24769432],\n",
       "       [12.10901093],\n",
       "       [12.31716669],\n",
       "       ...,\n",
       "       [12.49312952],\n",
       "       [11.86446223],\n",
       "       [11.90158345]])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/suzukiyuuta/Downloads/train.csv')#教師データを読み込む\n",
    "df_X_ori = np.log(df.loc[:,['GrLivArea','YearBuilt']].values)\n",
    "df_Y_ori = np.log(df.loc[:,['SalePrice']].values)\n",
    "\n",
    "df_X_ori.shape\n",
    "df_Y_ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 19.4198, val_loss : 1382.9233\n",
      "Epoch 1, loss : 3.8362, val_loss : 501.0757\n",
      "Epoch 2, loss : 1.2261, val_loss : 50.7540\n",
      "Epoch 3, loss : 0.7092, val_loss : 39.9238\n",
      "Epoch 4, loss : 0.4768, val_loss : 35.1797\n",
      "Epoch 5, loss : 0.4551, val_loss : 32.3300\n",
      "Epoch 6, loss : 0.5303, val_loss : 31.0605\n",
      "Epoch 7, loss : 0.5842, val_loss : 30.3951\n",
      "Epoch 8, loss : 0.6696, val_loss : 29.5379\n",
      "Epoch 9, loss : 0.6450, val_loss : 28.9352\n",
      "test_MSE : 52.161\n"
     ]
    }
   ],
   "source": [
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X_ori, df_Y_ori, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001 #学習率\n",
    "batch_size = 10 #バッチサイズ\n",
    "num_epochs = 10 #エポック数\n",
    "n_hidden1 = 50 #２層目\n",
    "n_hidden2 = 100 #１層目\n",
    "n_input = X_train.shape[1] #特徴量\n",
    "n_samples = X_train.shape[0] #サンプル数\n",
    "n_classes = 1 #２値分類（１個のワンホット）\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input]) #（サンプル数未定、特徴量数）\n",
    "Y = tf.placeholder(\"float\", [None, n_classes]) #（サンプル数未定、クラス分類）\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size) #ミニバッチでデータを分割\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])), #重み（４、５０）\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])), #重み（５０、１００）\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes])) #重み（１００、３）\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])), #バイアス（５０）\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),#バイアス（１００）\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))#バイアス（３）\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1']) #X@W1 + b1\n",
    "    layer_1 = tf.nn.relu(layer_1) #reru\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2']) #X@W2 + b2\n",
    "    layer_2 = tf.nn.relu(layer_2) #reru\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] #X@W3 + b3\n",
    "    return layer_output\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X) #上記クラスの計算\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_sum(tf.math.square(Y - logits))\n",
    "# 出力結果と正解値で交差エントロピー\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)#アダムグラッド？\n",
    "train_op = optimizer.minimize(loss_op)#クロスエントロピーの結果からバックプロパゲーション？\n",
    "# 推定結果\n",
    "# correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))#正解、不正解のBoolを返す\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_sum(tf.math.square(Y - logits))#１、０に変換し正解率を計算\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()#W,Bの初期化\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)#初期化\n",
    "    for epoch in range(num_epochs):#エポック数分最適化を繰り返す\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int) #小数点以下切り下げ？でバッチのループ数計算\n",
    "        total_loss = 0 #ロスを初期化\n",
    "        total_acc = 0 #正解率を初期化\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):#ミニバッチの計算\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y}) \n",
    "            #最適化の数値としてtrain_opを指定、引数として辞書型のミニバッチ（）\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #ロスエントロピーとアキュラシーをX,Yに対して計算\n",
    "            total_loss += loss #ロスをトータルロスに追加 \n",
    "            total_acc += acc #アキュラシーをトータルアキュラシーに追加\n",
    "        total_loss /= n_samples #サンプル数で割る\n",
    "        total_acc /= n_samples #サンプル数で割る\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})#バリデーションでバルロスを計算\n",
    "#         print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, loss, val_loss,))\n",
    "        # エポックごとにロス、バルロス、正解率をプリント\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})#テストデータで正解率を計算\n",
    "    print(\"test_MSE : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】MNISTのモデルを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max()) # 1.0\n",
    "print(X_train.min()) # 0.0\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "print(y_train.shape) # (60000,)\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) # float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 1, 28, 28)\n",
      "(59400, 1, 28, 28)\n",
      "(600, 10)\n",
      "(59400, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train_0, X_val_0, y_train_0, y_val_0 = train_test_split(X_train, y_train_one_hot, test_size=0.99)\n",
    "\n",
    "X_train_0 = X_train_0[:, np.newaxis, :, :]\n",
    "X_val_0 = X_val_0[:, np.newaxis, :, :]\n",
    "\n",
    "print(X_train_0.shape) # (600, 28,28,1)\n",
    "print(X_val_0.shape) # (59400, 28,28,1)\n",
    "print(y_train_0.shape) # (600, 10)\n",
    "print(y_val_0.shape) # (59400, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "step= 0 loss= 480.89835 acc= 0.1064\n",
      "step= 100 loss= 50.972904 acc= 0.8586\n",
      "step= 200 loss= 24.164627 acc= 0.9121\n",
      "step= 300 loss= 26.35611 acc= 0.9306\n",
      "step= 400 loss= 34.81744 acc= 0.9384\n",
      "step= 500 loss= 9.854622 acc= 0.9417\n",
      "step= 600 loss= 23.888836 acc= 0.9505\n",
      "step= 700 loss= 18.001362 acc= 0.9503\n",
      "step= 800 loss= 10.534403 acc= 0.954\n",
      "step= 900 loss= 8.4989395 acc= 0.9623\n",
      "正解率= 0.9648\n"
     ]
    }
   ],
   "source": [
    "# learning_rate = 0.1 #学習率\n",
    "# batch_size = 10 #バッチサイズ\n",
    "# num_epochs = 20 #エポック数\n",
    "# n_hidden1 = 50 #２層目\n",
    "# n_hidden2 = 100 #１層目\n",
    "# n_input = X_train.shape[1] #特徴量\n",
    "# n_samples = X_train.shape[0] #サンプル数\n",
    "# n_classes = 10 #回帰のため１\n",
    "\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets(\"mnist/\", one_hot=True)\n",
    "\n",
    "# print(mnist.test.images.shape)\n",
    "# print(mnist.test.labels.shape)\n",
    "\n",
    "pixels = 28\n",
    "nums = 10\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(tf.float32, shape=(None,pixels), name=\"x\") #（サンプル数未定、特徴量数）\n",
    "Y = tf.placeholder(tf.float32, shape=(None,nums), name=\"y_\") #（サンプル数未定、クラス分類）\n",
    "\n",
    "def weight_variable(name,shape):\n",
    "    W_init = tf.truncated_normal(shape, stddev=0.1)\n",
    "    W = tf.Variable(W_init, name=\"W\"+name)\n",
    "    return W\n",
    "def bias_variable(name,size):\n",
    "    b_init = tf.constant(0.1, shape=[size])\n",
    "    b = tf.Variable(b_init, name=\"b_\"+name)\n",
    "    return b\n",
    "\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding=\"SAME\")\n",
    "def max_pool(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "with tf.name_scope(\"conv1\")as scope:\n",
    "    W_conv1 = weight_variable(\"conv1\",[5,5,1,32])\n",
    "    b_conv1 = bias_variable(\"conv1\",32)\n",
    "    x_image = tf.reshape(x,[-1,28,28,1])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "with tf.name_scope(\"pool1\")as scope:\n",
    "    h_pool1 = max_pool(h_conv1)\n",
    "\n",
    "with tf.name_scope(\"conv2\")as scope:\n",
    "    W_conv2 = weight_variable(\"conv2\",[5,5,32,64])\n",
    "    b_conv2 = bias_variable(\"conv2\",64)\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    \n",
    "with tf.name_scope(\"pool2\")as scope:\n",
    "    h_pool2 = max_pool(h_conv2)\n",
    "\n",
    "with tf.name_scope(\"fully_connected\")as scope:\n",
    "    n = 7*7*64\n",
    "    W_fc = weight_variable(\"fc\",[n,1024])\n",
    "    b_fc = bias_variable(\"fc\",1024)\n",
    "    h_pool2_flat = tf.reshape(h_pool2,[-1,n])\n",
    "    h_fc = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc) + b_fc)\n",
    "    \n",
    "with tf.name_scope(\"dropout\")as scope:\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc_drop = tf.nn.dropout(h_fc, keep_prob)\n",
    "    \n",
    "with tf.name_scope(\"readout\")as scope:\n",
    "    W_fc2 = weight_variable(\"fc2\",[1024,10])\n",
    "    b_fc2 = bias_variable(\"fc2\",10)\n",
    "    y_conv = tf.nn.softmax(tf.matmul(h_fc_drop, W_fc2) + b_fc2)\n",
    "\n",
    "with tf.name_scope(\"loss\")as scope:\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "\n",
    "with tf.name_scope(\"training\")as scope:\n",
    "    optimizer = tf.train.AdamOptimizer(0.0001)\n",
    "    train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope(\"predict\")as scope:\n",
    "    predict_step = tf.equal(tf.argmax(y_conv,1),tf.argmax(y_,1))\n",
    "    accuracy_step = tf.reduce_mean(tf.cast(predict_step,tf.float32))\n",
    "\n",
    "def set_feed(images, labels, prob):\n",
    "    return{x: images, y_: labels, keep_prob: prob}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    tw = tf.summary.FileWriter('log_dir',graph=sess.graph)\n",
    "    test_fd = set_feed(mnist.test.images, mnist.test.labels,1)\n",
    "    for step in range(1000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        fd = set_feed(batch[0],batch[1],0.5)\n",
    "        _,loss = sess.run([train_step, cross_entropy], feed_dict=fd)\n",
    "        if step % 100 == 0:\n",
    "            acc = sess.run(accuracy_step, feed_dict=test_fd)\n",
    "            print(\"step=\", step,\"loss=\",loss,\"acc=\",acc)\n",
    "    acc = sess.run(accuracy_step, feed_dict=test_fd)\n",
    "    print(\"正解率=\",acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9139\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 画像データ\n",
    "x = tf.placeholder(\"float\", [None, 784])\n",
    "\n",
    "# モデルの重み\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "\n",
    "# モデルのバイアス\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# トレーニングデータxとモデルの重みWを乗算した後、モデルのバイアスbを足し、\n",
    "# ソフトマックス回帰（ソフトマックス関数）を適用\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# 正解データ\n",
    "y_ = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "# 損失関数をクロスエントロピーとする\n",
    "cross_entropy = -tf.reduce_sum(y_ * tf.log(y))\n",
    "\n",
    "# 学習係数を0.01として、勾配降下アルゴリズムを使用して、\n",
    "# クロスエントロピーを最小化する\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "# 変数の初期化\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# セッションの作成\n",
    "sess = tf.Session()\n",
    "\n",
    "# セッションの開始および初期化の実行\n",
    "sess.run(init)\n",
    "\n",
    "# トレーニングの開始\n",
    "for i in range(1000):\n",
    "    # トレーニングデータからランダムに100個抽出する\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "\n",
    "    # 確率的勾配降下によりクロスエントロピーを最小化するよう重みを更新\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "# 予測値と正解値を比較して、bool値（true or false）にする\n",
    "# tf.argmax(y, 1)は、予測値の各行で、最大値となるインデックスを一つ返す\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "\n",
    "# bool値を0 or 1に変換して平均値をとる -> 正解率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "# テストデータを与えて、テストデータの正解率の表示\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9168\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Mnistに使うデータセットをインポートする\n",
    "mnist = input_data.read_data_sets('/tmp/tensorflow/mnist/input_data',\n",
    "  one_hot=True)\n",
    "\n",
    "# グラフの作成\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # 入力するPlaceholder\n",
    "W = tf.Variable(tf.zeros([784, 10])) # 重み\n",
    "b = tf.Variable(tf.zeros([10])) # バイアス\n",
    "y = tf.matmul(x, W) + b #内積計算とバイアスの加算\n",
    "\n",
    "# 正解ラベル用のplaceholeder\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# 損失の計算方法と、オプティマイザーを定義\n",
    "cross_entropy = tf.reduce_mean(\n",
    " tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# セッションの作成と初期化\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    " \n",
    "# 学習部(1000回学習)\n",
    "for _ in range(1000):\n",
    " batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    " sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "# テスト部\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images,\n",
    " y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "0.9159\n"
     ]
    }
   ],
   "source": [
    "# データのインポート\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    " \n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "sess = tf.InteractiveSession()\n",
    " \n",
    "# モデルの作成\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    " \n",
    "# 損失とオプティマイザーを定義\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = 　　　　　　　　　　　　\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    " \n",
    "# 訓練\n",
    "tf.initialize_all_variables().run()\n",
    "for i in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  train_step.run({x: batch_xs, y_: batch_ys})\n",
    " \n",
    "# 訓練モデルのテスト\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
